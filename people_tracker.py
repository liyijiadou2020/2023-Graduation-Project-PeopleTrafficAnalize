import os
from collections import deque
import time
import cv2
import numpy as np
import torch
import warnings
import argparse
from person_count import tlbr_midpoint, intersect, vector_angle, get_size_with_pil, compute_color_for_labels, \
    put_text_to_cv2_img_with_pil, draw_yellow_line, makedir, print_statistics_to_frame, print_newest_info, \
    draw_idx_frame
from utils.datasets import LoadStreams, LoadImages
from utils.draw import draw_boxes_and_text, draw_reid_person, draw_boxes
from utils.general import check_img_size
from person_detect_yolov5 import YoloPersonDetect
from deep_sort import build_tracker, DeepReid
from utils.parser import get_config
from utils.log import get_logger
from utils.torch_utils import select_device, load_classifier, time_synchronized
from sklearn.metrics.pairwise import cosine_similarity
from fast_reid.demo.person_bank import Reid_feature
from pathlib import Path
from pycallgraph2 import PyCallGraph
from pycallgraph2.output import GraphvizOutput


class VideoStreamTracker():
    def __init__(self, yolo_model,
                 reid_model,
                 deepsort_model,
                 dataset,
                 query_feat,
                 query_names,
                 output_people_img_path,
                 is_display, p1, p2, tracker_type_number=-1):
        '''
        todo: é»˜è®¤å‚æ•°ï¼šcus_features - None, cus_names - [],  is_display - True
        parameters:
            cus_features : reidä½¿ç”¨çš„æŸ¥è¯¢åº“çš„ç‰¹å¾
            output_people_img_path : å°†æå–å‡ºçš„äººç‰©å›¾åƒæ”¾åœ¨ä»€ä¹ˆè·¯å¾„ä¸‹
            is_display : æ˜¯å¦å±•ç¤ºè§†é¢‘
            p1, p2 : é»„çº¿çš„ä¸¤ä¸ªç«¯ç‚¹å ç”»é¢çš„æ¯”ä¾‹ï¼ˆ0-1ä¹‹é—´ï¼‰ï¼Œç”¨åˆ—è¡¨çš„å½¢å¼ä¼ é€’
            tracker_type_number : 0ä»£è¡¨å…¥å£æ‘„åƒå¤´ï¼Œ 1ä»£è¡¨å‡ºå£æ‘„åƒå¤´ï¼Œ å…¶ä»–æ•°å­—ä»£è¡¨åº—å†…æ‘„åƒå¤´
        '''
        self.idx_frame = 0
        self.total_track = 0
        self.total_counter = 0
        self.up_count = 0
        self.down_count = 0
        self.already_counted = deque(maxlen=100)
        self.yolo_model = yolo_model
        self.deepsort = deepsort_model
        self.dataset = dataset
        self.tracker_type_number = tracker_type_number
        # 1.ç”»é»„çº¿
        self.p1_ratio = p1
        self.p2_ratio = p2
        # 2.å¤„ç†tracks
        self._save_dir = output_people_img_path
        # 3.ReID
        self.reid_model = reid_model
        self.query_feat = query_feat
        self.query_names = query_names
        # 4.ç»˜åˆ¶ç»Ÿè®¡ä¿¡æ¯ & ç»˜åˆ¶æ£€æµ‹æ¡† & ç»˜åˆ¶å¸§æ•°
        # 5.å±•ç¤ºå›¾åƒï¼Œè¾“å‡ºç»“æœè§†é¢‘
        self.is_display = is_display
        self.is_save_vid = False # todo: å¢åŠ åˆ°å‚æ•°ä¸­
        # 6.é”€æ¯çª—å£ & æ‰“å°log

    def tracking(self, query_feat=None, query_names=[]):
        # å¦‚æœä¸æ˜¯å…¥å£æ‘„åƒå¤´ï¼Œé‚£ä¹ˆåœ¨å¤„ç†ä¹‹å‰è¦æ›´æ–°ä¸€ä¸‹query_feat, cus_names
        if self.tracker_type_number != 0:
            self.update_reid_query(query_feat, query_names)

        paths = {}  # æ¯ä¸€ä¸ªtrackçš„è¡ŒåŠ¨è½¨è¿¹
        last_track_id = -1
        angle = -1
        already_counted = deque(maxlen=100)  # temporary memory for storing counted IDs

        vid_path = None
        vid_writer = None

        for video_path, img, ori_img, vid_cap in self.dataset:  # è·å–è§†é¢‘å¸§
            self.idx_frame += 1
            start_time = time_synchronized()

            # yolo detection
            bbox_xywh, cls_conf, cls_ids, xy = self.yolo_model.detect(video_path, img, ori_img, vid_cap)
            # outputs, features = self.deepsort.update(bbox_xywh, cls_conf, ori_img)
            if len(bbox_xywh) > 0 and len(cls_conf) > 0: # åŠ ä¸Šè¿™å¥ï¼Œå¦‚æœæ²¡æ£€æµ‹åˆ°äººå°±ç›´æ¥è·³è¿‡è¿™ä¸€å¸§
                outputs, features = self.deepsort.update(bbox_xywh, cls_conf, ori_img)

            # 1.ç”»é»„çº¿
            yellow_line = draw_yellow_line(self.p1_ratio, self.p2_ratio, ori_img)
            # 2. å¤„ç†tracks
            for track in outputs:
                bbox = track[:4]
                track_id = track[-1]
                midpoint_1 = tlbr_midpoint(bbox) # TODO: ç®€åŒ–æ’çº¿è®¡ç®—
                origin_midpoint = (midpoint_1[0],
                                   ori_img.shape[0] - midpoint_1[1])  # get midpoint_1 respective to bottom-left
                if track_id not in paths:
                    paths[track_id] = deque(maxlen=2)  # pathä¿å­˜äº†æ¯ä¸ªtrackçš„ä¸¤ä¸ªå¸§çš„midpointï¼ˆè¿åŠ¨è½¨è¿¹ï¼‰
                    total_track = track_id
                paths[track_id].append(midpoint_1)
                midpoint_0 = paths[track_id][0]  # æ­¤trackå‰ä¸€å¸§çš„midpoint
                origin_previous_midpoint = (midpoint_0[0], ori_img.shape[0] - midpoint_0[1])

                if intersect(midpoint_1, midpoint_0, yellow_line[0], yellow_line[1]) \
                        and track_id not in already_counted:
                    self.total_counter += 1
                    last_track_id = track_id;  # è®°å½•è§¦çº¿è€…çš„ID
                    cv2.line(ori_img, yellow_line[0], yellow_line[1], (0, 0, 255), 1)  # è§¦ç¢°çº¿çš„æƒ…å†µä¸‹ç”»çº¢çº¿
                    already_counted.append(track_id)  # Set already counted for ID to true.
                    angle = vector_angle(origin_midpoint, origin_previous_midpoint)  # è®¡ç®—è§’åº¦ï¼Œåˆ¤æ–­å‘ä¸Šè¿˜æ˜¯å‘ä¸‹èµ°
                    if angle > 0:  # è¿›åº—
                        self.up_count += 1
                        if self.tracker_type_number == 0:
                            self.customer_enter(bbox, ori_img, track_id, yellow_line)
                        else: # åŠŸèƒ½å·²ç»å®ç°ï¼Œä½†æ˜¯è¦è°ƒå‚
                            self.person_search(bbox, ori_img, track_id)
                    if angle < 0:
                        self.down_count += 1

            # 3.é‡è¯†åˆ«ç»“æœ - Enteræ‘„åƒå¤´ä¸éœ€è¦ç®¡è¿™ä¸ª
            if self.tracker_type_number != 0:
                img, match_names = self.draw_reid_result_to_frame(features, ori_img, xy)
            # 4.ç»˜åˆ¶ç»Ÿè®¡ä¿¡æ¯
            ori_img = self.draw_info_to_frame(angle, last_track_id, ori_img, outputs, self.total_track)
            # 5.å±•ç¤ºå›¾åƒï¼Œè¾“å‡ºç»“æœè§†é¢‘
            if self.is_display:
                cv2.imshow("test", ori_img)
                if cv2.waitKey(1) & 0xFF == 27:
                    break
            end_time = time_synchronized()
            if self.is_save_vid: # è¾“å‡ºè§†é¢‘
                if vid_path != self._save_dir:
                    vid_path = self._save_dir
                    if isinstance(vid_writer, cv2.VideoWriter):
                        vid_writer.release()
                    if vid_cap:  # video
                        fps = vid_cap.get(cv2.CAP_PROP_FPS)
                        w = int(vid_cap.get(cv2.CAP_PROP_FRAME_WIDTH))
                        h = int(vid_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
                    else:  # stream
                        fps, w, h = 30, ori_img.shape[1], ori_img.shape[0]
                    save_path = str(
                        Path(self._save_dir).with_suffix('.mp4'))  # force *.mp4 suffix on results videos
                    vid_writer = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))
                vid_writer.write(ori_img)
            print("Index of frame: {} "
                              "Spend time: {:.03f}s, "
                              "Fps: {:.03f}, "
                              "Tracks : {}, "
                              "Detections : {}, "
                              "Features of detections: {}"
                              .format(self.idx_frame
                                      # , self.total_frame_count
                                      , end_time - start_time
                                      , 1 / (end_time - start_time)
                                      , bbox_xywh.shape[0]
                                      , len(outputs)
                                      , len(bbox_xywh)
                                      , features.shape
                                      )
                              )
        cv2.destroyAllWindows()  ## é”€æ¯æ‰€æœ‰opencvæ˜¾ç¤ºçª—å£

        if self.tracker_type_number == 0: # å¦‚æœè¿™æ˜¯å…¥å£æ‘„åƒå¤´ï¼Œéœ€è¦æå–ç‰¹å¾åç»­ä½¿ç”¨
            return self.feature_extract()
        # vid_writer.release()

    def customer_enter(self, bbox, ori_img, track_id, yellow_line_in):
        # todo: æŠŠæ’çº¿äººçš„ç‰¹å¾è¾“å‡ºæ¥ & è®°å½•å…¥åº—çš„æ—¶é—´

        # è¿›åº—çš„æ—¶å€™ï¼ŒæŠŠäººç‰©çš„å›¾åƒæŠ å‡ºæ¥
        cv2.line(ori_img, yellow_line_in[0], yellow_line_in[1], (0, 0, 0), 1)  # æ¶ˆé™¤çº¿æ¡
        ROI_person = ori_img[int(bbox[1]):int(bbox[3]), int(bbox[0]):int(bbox[2])]
        path = str(self._save_dir + '/cus{}.jpg'.format(track_id))
        makedir(path)
        cv2.imwrite(path, ROI_person)
        # æ‰“å°å½“å‰çš„æ—¶é—´ & é¡¾å®¢å…¥åº—ä¿¡æ¯
        current_time = int(time.time())
        localtime = time.localtime(current_time)
        dt = time.strftime('%Y-%m-%d %H:%M:%S', localtime)
        print("[Customer cameğŸ‘] current customerğŸ’‚â€â™‚ï¸: {}, "
              "Enter timeâ° : {}".format(
            track_id
            , dt
        ))

    def person_search(self, bbox, ori_img, track_id): # todo: bug, é‡è¯†åˆ«å¤±è´¥äº†ï¼Œwhyï¼Ÿ
        ROI_person = ori_img[int(bbox[1]):int(bbox[3]), int(bbox[0]):int(bbox[2])]
        query_feature_vector = self.reid_model(ROI_person)
        # ---- compare -----------
        cos_sim = cosine_similarity(self.query_feat, query_feature_vector)
        max_idx = np.argmax(cos_sim, axis=1)  # æ¯è¡Œæœ€å¤§å€¼çš„ç´¢å¼•
        maximum = np.max(cos_sim, axis=1)
        print("[ReID DEBUG] maximum = ", maximum)
        # --- fix bug: å¦‚æœæœç´¢ä¸åˆ° ---
        if max(maximum) > 0.4: # 0.5å¤§äº†
            max_idx[maximum < 0.4] = -1
            idx = np.argmax(max_idx)  # todo: bug, æ²¡åŠæ³•æ£€æµ‹å…¨æ˜¯-1çš„æƒ…å†µ
            person_name = self.query_names[idx]  # æœå¯»å¾—åˆ°çš„
        else:
            person_name = "new-{}".format(track_id)

        print("[DEBUG-reid] person_name: ", person_name)
        path = str(self._save_dir + '/{}.jpg'.format(person_name))
        makedir(path)
        cv2.imwrite(path, ROI_person)

        # æ‰“å°å½“å‰çš„æ—¶é—´ & é¡¾å®¢å…¥åº—ä¿¡æ¯
        current_time = int(time.time())
        localtime = time.localtime(current_time)
        dt = time.strftime('%Y-%m-%d %H:%M:%S', localtime)
        print("[ReID ResultğŸ™Œ] person: {}, "
              "timeâ° : {}".format(
            person_name
            , dt
        ))




    def draw_info_to_frame(self, angle, last_track_id, ori_img, outputs, total_track):
        ori_img = print_statistics_to_frame(self.down_count, ori_img, self.total_counter, total_track, self.up_count)
        ori_img = draw_idx_frame(ori_img, self.idx_frame)
        if last_track_id >= 0:
            ori_img = print_newest_info(angle, last_track_id, ori_img)  # æ‰“å°æ’çº¿äººçš„ä¿¡æ¯
        if len(outputs) > 0:  # å±•ç¤ºè·Ÿè¸ªç»“æœ
            bbox_tlwh = []
            bbox_xyxy = outputs[:, :4]
            identities = outputs[:, -1]
            draw_boxes_and_text(ori_img, bbox_xyxy, identities)  # ç»™æ¯ä¸ªdetectionç”»æ¡†
            for bb_xyxy in bbox_xyxy:
                bbox_tlwh.append(self.deepsort._xyxy_to_tlwh(bb_xyxy))
        return ori_img

    def draw_reid_result_to_frame(self, features, ori_img, xy):
        person_cossim = cosine_similarity(features, self.query_feat)  # è®¡ç®—featureså’Œquery_featuresçš„ä½™å¼¦ç›¸ä¼¼åº¦
        max_idx = np.argmax(person_cossim, axis=1)
        maximum = np.max(person_cossim, axis=1)
        max_idx[maximum < 0.5] = -1
        score = maximum
        reid_results = max_idx
        img, match_names = draw_reid_person(ori_img, xy, reid_results, self.query_names)  # draw_person name
        print("[ReID] match people names: {} .".format(match_names))
        return img, match_names

    def update_reid_query(self, features, names):
        self.query_feat = features
        self.query_names = names

    def feature_extract(self):
        # reid_feature = Reid_feature() # reid model
        names = []
        embs = np.ones((1, 512), dtype=np.int)
        for image_name in os.listdir(self._save_dir):
            img = cv2.imread(os.path.join(self._save_dir, image_name))
            feat = self.reid_model(img)  # æå–ç‰¹å¾ï¼Œè¿”å›æ­£åˆ™åŒ–çš„numpyæ•°ç»„
            pytorch_output = feat.numpy()  # è½¬åŒ–æˆnumpyæ•°ç»„
            embs = np.concatenate((pytorch_output, embs), axis=0) # å’Œå·²æœ‰çš„ç‰¹å¾å‘é‡æ•°ç»„embsåœ¨ç¬¬0ç»´ä¸Šè¿›è¡Œæ‹¼æ¥ï¼Œå¾—åˆ°æ›´æ–°åçš„embsæ•°ç»„
            names.append(image_name[0:-4])  # å»é™¤.jpgä½œä¸ºé¡¾å®¢çš„åå­—
        names = names[::-1] # å€’åºç¿»è½¬ [1, 2, 3, 4, 5] -> [5, 4, 3, 2, 1]
        names.append("None")

        feat_path = os.path.join(str(self._save_dir), '..', 'query_features')
        names_path = os.path.join(str(self._save_dir), '..', 'names')
        # å®é™…ä¿å­˜çš„æ˜¯embsæ•°ç»„ä¸­é™¤äº†æœ€åä¸€è¡Œä»¥å¤–çš„æ‰€æœ‰è¡Œ
        np.save(feat_path, embs[:-1, :]) # å°†numpyæ•°ç»„ embs çš„ç¬¬ä¸€ç»´åº¦çš„æ‰€æœ‰å…ƒç´ ï¼ˆé™¤äº†æœ€åä¸€ä¸ªå…ƒç´ ï¼‰ä¿å­˜ä¸ºäºŒè¿›åˆ¶æ–‡ä»¶çš„æ“ä½œï¼Œæ–‡ä»¶åä¸º feat_path
        np.save(names_path, names)  # save query

        # ä»£ç è¯»å–ä¸€ä¸ª.npyæ–‡ä»¶ï¼Œè¯¥æ–‡ä»¶ä¸­åŒ…å«äº†ä¸€ä¸ªç‰¹å¾å‘é‡queryï¼Œå°†å¦å¤–ä¸€ç»„ç‰¹å¾å‘é‡embsä¸queryè®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        path = '{}.npy'.format(str(feat_path))
        query = np.load(path)
        cos_sim = cosine_similarity(embs, query)
        max_idx = np.argmax(cos_sim, axis=1)
        maximum = np.max(cos_sim, axis=1)
        max_idx[maximum < 0.6] = -1
        # -------------- ç”¨ä½œreidå¯¹æ¯” ï¼Ÿ --------------
        print("Succeed extracting features for ReID.")

        return query, names